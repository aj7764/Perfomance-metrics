{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmujI9fPsi5a",
        "outputId": "602663bb-c152-46c5-cf95-fb6d5cf8eb11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-score: 0.0\n",
            "Confusion Matrix:\n",
            "True Positives: 0\n",
            "False Positives: 0\n",
            "True Negatives: 686\n",
            "False Negatives: 0\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset, assuming it has 'actual_labels' and 'predicted_labels' columns\n",
        "data = pd.read_csv(\"data_moods.csv\")\n",
        "\n",
        "# Get the actual and predicted labels\n",
        "actual_labels = data['acousticness']\n",
        "predicted_labels = data['mood']\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "total_predictions = len(actual_labels)\n",
        "if total_predictions == 0:\n",
        "    accuracy = 0.0\n",
        "else:\n",
        "    correct_predictions = sum(actual == predicted for actual, predicted in zip(actual_labels, predicted_labels))\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "# Calculate precision\n",
        "true_positives = sum((actual == 1) and (predicted == 1) for actual, predicted in zip(actual_labels, predicted_labels))\n",
        "false_positives = sum((actual == 0) and (predicted == 1) for actual, predicted in zip(actual_labels, predicted_labels))\n",
        "if true_positives + false_positives == 0:\n",
        "    precision = 0.0\n",
        "else:\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "\n",
        "# Calculate recall\n",
        "false_negatives = sum((actual == 1) and (predicted == 0) for actual, predicted in zip(actual_labels, predicted_labels))\n",
        "if true_positives + false_negatives == 0:\n",
        "    recall = 0.0\n",
        "else:\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "\n",
        "# Calculate F1-score\n",
        "if precision + recall == 0:\n",
        "    f1_score = 0.0\n",
        "else:\n",
        "    f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "confusion_matrix = {\n",
        "    \"True Positives\": true_positives,\n",
        "    \"False Positives\": false_positives,\n",
        "    \"True Negatives\": total_predictions - (true_positives + false_positives + false_negatives),\n",
        "    \"False Negatives\": false_negatives,\n",
        "}\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1_score}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "for key, value in confusion_matrix.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    }
  ]
}